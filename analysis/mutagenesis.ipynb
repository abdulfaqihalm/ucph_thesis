{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from model import ConfigurableModel\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logomaker\n",
    "import os\n",
    "from typing import Callable\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/binf-isilon/renniegrp/vpx267/miniconda3/envs/thesis_ray_gpu/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "config = {\"cnn_first_filter\": 10, \"cnn_first_kernel_size\": 9, \"cnn_length\": 2, \"cnn_filter\": 64, \"cnn_kernel_size\": 5, \"bilstm_layer\": 3, \"bilstm_hidden_size\": 64, \"fc_size\": 256}\n",
    "model = ConfigurableModel(input_channel=4, cnn_first_filter=config[\"cnn_first_filter\"], cnn_first_kernel_size=config[\"cnn_first_kernel_size\"],\n",
    "                                cnn_length=config[\"cnn_length\"], cnn_other_filter=config[\"cnn_filter\"], cnn_other_kernel_size=config[\"cnn_kernel_size\"], bilstm_layer=config[\"bilstm_layer\"], bilstm_hidden_size=config[\"bilstm_hidden_size\"], fc_size=config[\"fc_size\"],\n",
    "                                output_size=2)\n",
    "#fold\n",
    "fold = 1\n",
    "model_weight = torch.load(f\"/binf-isilon/renniegrp/vpx267/ucph_thesis/data/outputs/models/trained_model_{fold}th_fold_dual_outputs_m6_info-no_promoter-False_ONE_HOT_BEST_PARAM.pkl\",\n",
    "                          map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_weight)\n",
    "sys.path.append(\"/binf-isilon/renniegrp/vpx267/ucph_thesis/wrapper\")\n",
    "from wrapper import utils\n",
    "seq_fasta_test_path = f\"/binf-isilon/renniegrp/vpx267/ucph_thesis/data/dual_outputs/motif_fasta_test_SPLIT_{fold}.fasta\"\n",
    "seq_fasta_one_hot = utils.create_seq_tensor(seq_fasta_test_path)\n",
    "meta_data_test_json_path = f\"/binf-isilon/renniegrp/vpx267/ucph_thesis/data/dual_outputs/test_meta_data_SPLIT_{fold}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot data size: 419.777MB\n",
      "model size: 62.971MB\n",
      "torch.Size([27483, 4, 1001])\n",
      "Estimated size of total mutated data: 1,680.786GB\n",
      "Estimated size of single mutated sequence: 1,680.786GB\n"
     ]
    }
   ],
   "source": [
    "one_hot_size_mb = sys.getsizeof(seq_fasta_one_hot.storage())/(1024**2)\n",
    "one_hot_size_single_mb = sys.getsizeof(seq_fasta_one_hot[0,:,:].storage())/(1024**2)\n",
    "# print(f\"one hot data size: {one_hot_size_mb:,.3f}MB\")\n",
    "# param_size = 0\n",
    "# for param in model.parameters():\n",
    "#     param_size += param.nelement() * param.element_size()\n",
    "# buffer_size = 0\n",
    "# for buffer in model.buffers():\n",
    "#     buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "# size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "# print(f\"model size: {size_all_mb:,.3f}MB\")\n",
    "# print(seq_fasta_one_hot.shape)\n",
    "# # Point wise mutation will generate 4*seq_length new sequences\n",
    "# print(f\"Estimated size of total mutated data: {one_hot_size_mb*4*seq_fasta_one_hot.shape[2]*0.001:,.3f}GB\")\n",
    "# print(f\"Estimated size of single mutated sequence: {one_hot_size_single_mb*4*seq_fasta_one_hot.shape[2]*0.001:,.3f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the size is too much for bulk analysis if we want to mutate all of the possible bases for each position. Hence we just need to focus on location of interest i.e. 50 up-down stream. Let's make estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated size of fully mutated data: 167.911GB\n"
     ]
    }
   ],
   "source": [
    "slice_size = 100\n",
    "print(f\"Estimated size of fully mutated data: {one_hot_size_mb*4*slice_size*0.001:,.3f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutagenesis(x: torch.Tensor, model: torch.nn.Module, mutation_size :int=50, class_index: None|int=None, verbose: bool=True) -> np.ndarray:\n",
    "    \"\"\" \n",
    "    in silico mutagenesis\n",
    "\n",
    "    input: x: one-hot-encoded sequences with shape of (batch, seq_length, 4)\n",
    "    input: model: trained model \n",
    "\n",
    "    \"\"\"\n",
    "    device = (torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "            else torch.device(\"cpu\"))\n",
    "\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        if not x.is_cuda:\n",
    "            x = x.cuda()\n",
    "        if next(model.parameters()).device == torch.device(\"cpu\"): \n",
    "            model = model.cuda()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    def generate_mutagenesis(x: torch.Tensor, mutation_size: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create a single point mutation at each position of the input sequence. \n",
    "        \n",
    "        param: x: one-hot-encoded sequences with shape of (batch, seq_length, 4)\n",
    "        return: list of mutagenized one-hot-encoded sequences with shape of (batch*seq_length*4, seq_length, 4)  \n",
    "        \"\"\"\n",
    "        _,L,A = x.shape \n",
    "        mid = L//2\n",
    "        x_mut = np.zeros((mutation_size*2+1, L, A))\n",
    "        x_cpu = x.cpu().numpy()\n",
    "        k = 0\n",
    "        for l in tqdm(range(mid-mutation_size, mid+mutation_size+1)):\n",
    "            for a in range(A):\n",
    "                x_new = np.copy(x_cpu) # move to CPU if x in GPU\n",
    "                x_new[0,l,:] = 0\n",
    "                x_new[0,l,a] = 1\n",
    "                print(x_mut[k].shape)\n",
    "                print(x_new.shape)\n",
    "                x_mut[k] = x_new\n",
    "                k += 1\n",
    "        return x_mut\n",
    "\n",
    "    def reconstruct_map(x: torch.Tensor, predictions: np.ndarray, mutation_size: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reconstruct the mutagenezied score\n",
    "\n",
    "        param: x: one-hot-encoded sequences with shape of (batch, seq_length, 4)\n",
    "        predictions: model predictions with shape of (batch*seq_length*4)\n",
    "\n",
    "        return: reconstructed mut_score with shape of (1, seq_length, 4)\n",
    "        \"\"\"\n",
    "        _,L,A = x.shape \n",
    "        mid = L//2\n",
    "        \n",
    "        mut_score = np.zeros((1,L,A))\n",
    "        k = 0\n",
    "        for l in tqdm(range(mid-mutation_size, mid+mutation_size+1)):\n",
    "            for a in range(A):\n",
    "                mut_score[0,l,a] = predictions[k]\n",
    "                k += 1\n",
    "        return mut_score\n",
    "\n",
    "    def get_score(x: torch.Tensor, model: torch.nn.Module, class_index: int=None, batch_size=256) -> np.ndarray:\n",
    "      \"\"\"\n",
    "      Get score from the model and process based on class_index.\n",
    "      \"\"\"\n",
    "      x_loader = torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=False)\n",
    "      score = torch.Tensor().to(\"cpu\")\n",
    "\n",
    "      # Batching to prevent the GPU from running out of memory\n",
    "      epoch_iterator = tqdm(x_loader, desc=\"Data Loader Iteration\")\n",
    "      for _, data in enumerate(epoch_iterator):\n",
    "          pred = model.predict(data)\n",
    "          torch.cat([score, pred.cpu().detach()])\n",
    "      score = score.numpy()\n",
    "      if class_index == None:\n",
    "          # Square root of sum of squares of all classes\n",
    "          score = np.sqrt(np.sum(score**2, axis=-1, keepdims=True)) \n",
    "      else:\n",
    "          # Choosing class based on class_index\n",
    "          score = score[:,class_index]\n",
    "      return score.cpu().numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # generate mutagenized sequences\n",
    "        if verbose:\n",
    "            print(\"Generating mutagenized sequences...\")\n",
    "        x_mut = generate_mutagenesis(x, mutation_size)\n",
    "        \n",
    "        # get baseline wildtype score\n",
    "        if verbose:\n",
    "            print(\"Getting baseline wildtype score...\")\n",
    "        wt_score = get_score(x, model, class_index) # [batch_size,1] \n",
    "        predictions = get_score(x_mut, model, class_index) # [batch_size * 4, 1]\n",
    "\n",
    "        # reshape mutagenesis predictiosn\n",
    "        if verbose:\n",
    "            print(\"Reconstructing mutagenized score...\")\n",
    "        mut_score = reconstruct_map(x, predictions, mutation_size) # (1, seq_length, 4)\n",
    "\n",
    "    # Back to cpu\n",
    "    if device == torch.device(\"cuda\"):\n",
    "      x = x.cpu()\n",
    "      model = model.cpu()\n",
    "\n",
    "    return mut_score - wt_score # Non-mutagenized difference would be zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating mutagenized sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/101 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 4)\n",
      "(27483, 1001, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (27483,1001,4) into shape (1001,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ctrl_mutant_score_diff \u001b[38;5;241m=\u001b[39m \u001b[43mmutagenesis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_fasta_one_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# case_mutant_score_diff = mutagenesis(seq_fasta_one_hot, model, class_index=1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 88\u001b[0m, in \u001b[0;36mmutagenesis\u001b[0;34m(x, model, mutation_size, class_index, verbose)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating mutagenized sequences...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m x_mut \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_mutagenesis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutation_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# get baseline wildtype score\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[0;32mIn[13], line 39\u001b[0m, in \u001b[0;36mmutagenesis.<locals>.generate_mutagenesis\u001b[0;34m(x, mutation_size)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x_mut[k]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x_new\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 39\u001b[0m         \u001b[43mx_mut\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m x_new\n\u001b[1;32m     40\u001b[0m         k \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_mut\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (27483,1001,4) into shape (1001,4)"
     ]
    }
   ],
   "source": [
    "ctrl_mutant_score_diff = mutagenesis(x=seq_fasta_one_hot.transpose(1,2), model=model, class_index=0)\n",
    "# case_mutant_score_diff = mutagenesis(seq_fasta_one_hot, model, class_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mutagenesis(x: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a single point mutation at each position of the input sequence. \n",
    "    \n",
    "    param: x: one-hot-encoded sequences with shape of (batch, seq_length, 4)\n",
    "    return: list of mutagenized one-hot-encoded sequences with shape of (batch*seq_length*4, seq_length, 4)  \n",
    "    \"\"\"\n",
    "    _,L,A = x.shape \n",
    "    mid = L//2\n",
    "    x_mut = []\n",
    "    for l in range(L):\n",
    "        for a in range(A):\n",
    "            x_new = np.copy(x.cpu()) # move to CPU if x in GPU\n",
    "            x_new[0,l,:] = 0\n",
    "            x_new[0,l,a] = 1\n",
    "            x_mut.append(x_new)\n",
    "    return np.concatenate(x_mut, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 4])\n",
      "(28, 7, 4)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1,0,0,0],[0,1,0,0],[0,1,0,0],[0,1,0,0],[0,1,0,0],[0,1,0,0],[0,1,0,0]]])\n",
    "print(x.shape)\n",
    "# print(x)\n",
    "coba = generate_mutagenesis(x)\n",
    "print(coba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "print(sys.getsizeof(x.storage()))\n",
    "print(sys.getsizeof(torch.from_numpy(coba).storage()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_ray_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
