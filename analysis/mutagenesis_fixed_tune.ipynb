{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"/binf-isilon/renniegrp/vpx267/ucph_thesis/\")\n",
    "from model import ConfigurableModel\n",
    "from wrapper import utils\n",
    "from torch.utils.data import DataLoader\n",
    "from wrapper.data_setup import SequenceDatasetDual\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from wrapper import utils\n",
    "import copy\n",
    "import concurrent.futures\n",
    "from typing import Iterable\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutagenesis(x: torch.Tensor, model: torch.nn.Module, mutation_size :int=50, class_index: None|int=None, verbose: bool=True, batch_size = 1024, cuda_device: int=6) -> np.ndarray:\n",
    "    \"\"\" \n",
    "    in silico mutagenesis\n",
    "\n",
    "    input: x: one-hot-encoded sequences with shape of (batch, seq_length, 4)\n",
    "    input: model: trained model \n",
    "\n",
    "    \"\"\"\n",
    "    device = (torch.device(f\"cuda:{cuda_device}\") if torch.cuda.is_available()\n",
    "            else torch.device(\"cpu\"))\n",
    "\n",
    "    if device == torch.device(f\"cuda:{cuda_device}\"):\n",
    "        torch.set_default_device(f\"cuda:{cuda_device}\")\n",
    "        torch.cuda.set_device(f\"cuda:{cuda_device}\") \n",
    "        if not x.is_cuda:\n",
    "            x = x.cuda(cuda_device)\n",
    "        if next(model.parameters()).device == torch.device(\"cpu\"): \n",
    "            model = model.cuda(cuda_device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    def generate_mutagenesis(x: np.ndarray, mutation_size: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Assuming input sequence is odd, It will output single nucleotide mutation at each position of the input sequence except the middle position.\n",
    "        It requires the mutation size to be even.\n",
    "        \n",
    "        param: x_copy: one-hot-encoded sequences with shape of (1, seq_length, 4)\n",
    "        param: mutation_size: even number. Total mutation sites\n",
    "        \n",
    "        return: list of mutagenized one-hot-encoded sequences with shape of (1*mutation_size*3, seq_length, 4)  \n",
    "\n",
    "        Example\n",
    "        --------\n",
    "        from wrapper import utils\n",
    "        \n",
    "        temp_seq = \"ACATG\"\n",
    "        temp_seq_one_hot = torch.from_numpy(utils.one_hot(temp_seq))\n",
    "        temp_seq_one_hot = temp_seq_one_hot.T.unsqueeze(0)\n",
    "        muts = generate_mutagenesis(temp_seq_one_hot, mutation_size=2)\n",
    "        print([utils.one_hot_to_sequence(mut) for mut in muts])\n",
    "        \"\"\"\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x_copy = copy.deepcopy(x)\n",
    "            x_copy = x_copy.cpu().numpy()\n",
    "\n",
    "        _,L,D = x_copy.shape \n",
    "        mid = L//2\n",
    "\n",
    "        if mutation_size>(L-1):\n",
    "            raise ValueError(\"Mutation size should be less than half of the sequence length.\")\n",
    "        elif mutation_size%2!=0:\n",
    "            raise ValueError(\"Mutation size should be even number.\")\n",
    "        \n",
    "        x_mut = np.zeros((mutation_size*3, L, D))\n",
    "        k = 0\n",
    "        for l in range(mid-int(mutation_size/2), mid+int(mutation_size/2)+1):\n",
    "            if l==mid:\n",
    "                continue\n",
    "            for d in range(3):\n",
    "                if x_copy[0,l,d]==1:\n",
    "                    continue\n",
    "                x_new = copy.deepcopy(x_copy)\n",
    "                x_new[0,l,:] = 0\n",
    "                x_new[0,l,d] = 1\n",
    "                x_mut[k] = x_new\n",
    "                k += 1\n",
    "        return x_mut\n",
    "\n",
    "    def get_score(x: torch.Tensor, model: torch.nn.Module, class_index: int=None, batch_size=32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get score from the model and process based on class_index.\n",
    "\n",
    "        param: x: torch.Tensor: one-hot-encoded sequences with shape of (batch, seq_length, 4)\n",
    "        param: model: torch.nn.Module: trained model\n",
    "        param: class_index: int: class index to choose from the model output 0 (control) or 1 (case)\n",
    "\n",
    "        return: score: np.ndarray: model predictions with shape of (batch, 1)\n",
    "\n",
    "        Example\n",
    "        --------\n",
    "        get_score(seq_fasta_one_hot[0:30,:,:], model, class_index=0)\n",
    "        \"\"\"\n",
    "        if class_index not in [0,1]:\n",
    "            raise ValueError(\"class_index should be either 0 or 1.\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(f\"cuda:{cuda_device}\")\n",
    "            if not x.is_cuda:\n",
    "                x = x.cuda(cuda_device)\n",
    "            if next(model.parameters()).device == torch.device(\"cpu\"): \n",
    "                model = model.cuda(cuda_device)\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        \n",
    "        if x.shape[2]==4:\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "        x_loader = torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=False)\n",
    "        score = torch.Tensor().to(device)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for data in x_loader:\n",
    "            pred = model(data) # [batch_size, 2]\n",
    "            score = torch.cat([score, pred.detach()])\n",
    "        score = score.detach().cpu().numpy()\n",
    "        if class_index == None:\n",
    "            # Square root of sum of squares of all classes for each sequence\n",
    "            score = np.sqrt(np.sum(score**2, axis=-1, keepdims=True)) # [batch_size, 1]\n",
    "        else:\n",
    "            # Choosing class based on class_index\n",
    "            score = score[:,class_index]\n",
    "        return score\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # generate mutagenized sequences\n",
    "        if verbose:\n",
    "            print(\"Generating mutagenized sequences...\")\n",
    "        x_mut = generate_mutagenesis(x, mutation_size)\n",
    "        x_mut = torch.from_numpy(x_mut).float().to(device) # (mutation_size*3, seq_length, 4)  \n",
    "        \n",
    "        # get baseline wildtype score\n",
    "        if verbose:\n",
    "            print(\"Getting baseline wildtype score...\")\n",
    "        wt_score = get_score(x, model, class_index, batch_size) # [1,] \n",
    "        predictions = get_score(x_mut, model, class_index, batch_size) # [mutation_size * 3,]\n",
    "        delta = predictions - wt_score \n",
    "        # mutation_size = 4\n",
    "        # temp1 = torch.Tensor([[1,2,3,4,5,6,7,8,9,10,11,12]])\n",
    "        # temp1 = temp1.reshape(mutation_size,3).T\n",
    "        # print(temp1)\n",
    "        delta = delta.reshape(mutation_size,3).T # [3, mutation_size]\n",
    "\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOPING FOR REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generating mutagenesis for fold 1\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/binf-isilon/renniegrp/vpx267/miniconda3/envs/thesis_ray_gpu/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input test shape: torch.Size([27483, 4, 1001]) | Mutation size = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis Score fold 1: 100%|██████████| 27483/27483 [3:12:01<00:00,  2.39it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Starting generating mutagenesis for fold 2\n",
      "Loading model...\n",
      "Input test shape: torch.Size([27199, 4, 1001]) | Mutation size = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis Score fold 2: 100%|██████████| 27199/27199 [3:09:51<00:00,  2.39it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Starting generating mutagenesis for fold 3\n",
      "Loading model...\n",
      "Input test shape: torch.Size([24482, 4, 1001]) | Mutation size = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mutagenesis Score fold 3:  33%|███▎      | 8169/24482 [57:30<1:54:50,  2.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m case_mutant_score_diff \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m (input_seq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMutagenesis Score fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m     ctrl_mutant_score_diff\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmutagenesis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutation_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmutation_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m     case_mutant_score_diff\u001b[38;5;241m.\u001b[39mappend(mutagenesis(x\u001b[38;5;241m=\u001b[39minput_seq[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,:,:]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), model\u001b[38;5;241m=\u001b[39mmodel, mutation_size\u001b[38;5;241m=\u001b[39mmutation_size, class_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     28\u001b[0m ctrl_mutant_score_diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(np\u001b[38;5;241m.\u001b[39marray(ctrl_mutant_score_diff))\n",
      "Cell \u001b[0;32mIn[14], line 119\u001b[0m, in \u001b[0;36mmutagenesis\u001b[0;34m(x, model, mutation_size, class_index, verbose, batch_size, cuda_device)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating mutagenized sequences...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m x_mut \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_mutagenesis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutation_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m x_mut \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x_mut)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (mutation_size*3, seq_length, 4)  \u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# get baseline wildtype score\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 66\u001b[0m, in \u001b[0;36mmutagenesis.<locals>.generate_mutagenesis\u001b[0;34m(x, mutation_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m         x_new[\u001b[38;5;241m0\u001b[39m,l,d] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     65\u001b[0m         x_mut[k] \u001b[38;5;241m=\u001b[39m x_new\n\u001b[0;32m---> 66\u001b[0m         k \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_mut\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold in [1,2,3,4,5]:\n",
    "    print(f\"Starting generating mutagenesis for fold {fold}\")\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    config = {'lr': 0.001, 'weight_decay': 0.1, 'cnn_first_filter': 24, 'cnn_first_kernel_size': 7, 'cnn_length': 3, 'cnn_filter': 32, 'cnn_kernel_size': 7, 'bilstm_layer': 3, 'bilstm_hidden_size': 256, 'fc_size': 256}           \n",
    "    model = ConfigurableModel(input_channel=4, cnn_first_filter=config[\"cnn_first_filter\"], cnn_first_kernel_size=config[\"cnn_first_kernel_size\"],\n",
    "                        cnn_other_filter=config[\"cnn_filter\"], cnn_other_kernel_size=config[\"cnn_kernel_size\"], bilstm_layer=config[\"bilstm_layer\"], bilstm_hidden_size=config[\"bilstm_hidden_size\"], fc_size=config[\"fc_size\"],\n",
    "                        output_size=2)\n",
    "    model_weight = torch.load(f\"/binf-isilon/renniegrp/vpx267/ucph_thesis/data/outputs/models/trained_model_{fold}th_fold_dual_outputs_m6A_info-no_promoter-False_fixed_tune.pkl\",\n",
    "                            map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(model_weight)\n",
    "    model.eval()\n",
    "\n",
    "    seq_fasta_test_path = f\"/binf-isilon/renniegrp/vpx267/ucph_thesis/data/dual_outputs/motif_fasta_test_SPLIT_{fold}.fasta\"\n",
    "    seq_fasta_one_hot = utils.create_seq_tensor(seq_fasta_test_path)\n",
    "\n",
    "    seqs_number = seq_fasta_one_hot.shape[0]\n",
    "    input_seq = seq_fasta_one_hot[0:seqs_number,:,:]\n",
    "    mutation_size=500\n",
    "\n",
    "    print(f\"Input test shape: {input_seq.shape} | Mutation size = 500\")\n",
    "    ctrl_mutant_score_diff = []\n",
    "    case_mutant_score_diff = []\n",
    "    for i in tqdm(range (input_seq.shape[0]), desc=f\"Mutagenesis Score fold {fold}\"):\n",
    "        ctrl_mutant_score_diff.append(mutagenesis(x=input_seq[i:i+1,:,:].transpose(1,2), model=model, mutation_size=mutation_size, class_index=0, verbose=False))\n",
    "        case_mutant_score_diff.append(mutagenesis(x=input_seq[i:i+1,:,:].transpose(1,2), model=model, mutation_size=mutation_size, class_index=1, verbose=False))\n",
    "        \n",
    "    ctrl_mutant_score_diff = np.vstack(np.array(ctrl_mutant_score_diff))\n",
    "    case_mutant_score_diff = np.vstack(np.array(case_mutant_score_diff))\n",
    "\n",
    "    print(\"Saving...\")\n",
    "    np.savez(f\"/binf-isilon/renniegrp/vpx267/ucph_thesis/analysis/mutagenesis_score_fold_{fold}_fixed_tune.npz\", ctrl_mutant_score_diff=ctrl_mutant_score_diff, case_mutant_score_diff=case_mutant_score_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_ray_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
